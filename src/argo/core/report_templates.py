"""Professional CTI report templates for Orpheus output."""

import os
import logging
from datetime import datetime, timezone
from typing import List, Dict, Any, Optional
from pathlib import Path
import json

logger = logging.getLogger(__name__)


class CTIReportTemplate:
    """Base class for CTI report templates."""
    
    def __init__(self, actor: str, evidence: List[Dict[str, Any]], metadata: Dict[str, Any]):
        self.actor = actor
        self.evidence = evidence
        self.metadata = metadata
        self.timestamp = datetime.now(timezone.utc)
    
    def generate_header(self) -> str:
        """Generate report header with metadata."""
        header = f"""# Threat Actor Profile: {self.actor}

**Generated by:** Orpheus CTI Agent  
**Report Date:** {self.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}  
**Evidence Count:** {len(self.evidence)} items  
**Confidence Level:** {self._calculate_confidence_level()}  
**TLP:** {self.metadata.get('tlp', 'CLEAR')}  

---
"""
        return header
    
    def _calculate_confidence_level(self) -> str:
        """Calculate overall confidence level based on evidence scores."""
        if not self.evidence:
            return "UNKNOWN"
        
        scores = [e.get('confidence', 0.0) for e in self.evidence]
        avg_score = sum(scores) / len(scores)
        
        if avg_score >= 0.8:
            return "HIGH"
        elif avg_score >= 0.6:
            return "MEDIUM"
        elif avg_score >= 0.4:
            return "LOW"
        else:
            return "VERY LOW"
    
    def generate_executive_summary(self) -> str:
        """Generate executive summary section."""
        summary = """## Executive Summary

This report provides a comprehensive analysis of threat actor **{actor}** based on {evidence_count} evidence items from {document_count} source documents.

**Key Findings:**
- **Threat Level:** {threat_level}
- **Primary Techniques:** {primary_techniques}
- **Target Sectors:** {target_sectors}
- **Infrastructure:** {infrastructure}
- **Timeline:** {timeline}

**Confidence Assessment:** {confidence_assessment}

---
""".format(
            actor=self.actor,
            evidence_count=len(self.evidence),
            document_count=len(set(e.get('document_id') for e in self.evidence)),
            threat_level=self._assess_threat_level(),
            primary_techniques=self._extract_primary_techniques(),
            target_sectors=self._extract_target_sectors(),
            infrastructure=self._extract_infrastructure(),
            timeline=self._extract_timeline(),
            confidence_assessment=self._generate_confidence_assessment()
        )
        
        return summary
    
    def _assess_threat_level(self) -> str:
        """Assess overall threat level based on evidence."""
        if not self.evidence:
            return "UNKNOWN"
        
        # Count high-confidence evidence
        high_conf = len([e for e in self.evidence if e.get('confidence', 0) >= 0.8])
        total = len(self.evidence)
        
        if high_conf / total >= 0.8:
            return "HIGH - Extensive evidence with high confidence"
        elif high_conf / total >= 0.6:
            return "MEDIUM-HIGH - Good evidence with moderate confidence"
        elif high_conf / total >= 0.4:
            return "MEDIUM - Adequate evidence with mixed confidence"
        else:
            return "LOW - Limited evidence with low confidence"
    
    def _extract_primary_techniques(self) -> str:
        """Extract primary TTPs from evidence."""
        techniques = set()
        for e in self.evidence:
            if 'techniques' in e:
                techniques.update(e['techniques'])
        
        if not techniques:
            return "Not identified in current evidence"
        
        # Return top 3 techniques
        return ", ".join(list(techniques)[:3])
    
    def _extract_target_sectors(self) -> str:
        """Extract target sectors from evidence."""
        # This would be enhanced with sector extraction logic
        return "Analysis required - evidence insufficient for sector identification"
    
    def _extract_infrastructure(self) -> str:
        """Extract infrastructure indicators from evidence."""
        infrastructure = set()
        for e in self.evidence:
            text = e.get('snippet', '').lower()
            if 'c2' in text or 'command' in text or 'control' in text:
                infrastructure.add('C2 Infrastructure')
            if 'domain' in text or 'ip' in text or 'url' in text:
                infrastructure.add('Network Infrastructure')
            if 'malware' in text or 'payload' in text:
                infrastructure.add('Malware Infrastructure')
        
        if not infrastructure:
            return "Not identified in current evidence"
        
        return ", ".join(infrastructure)
    
    def _extract_timeline(self) -> str:
        """Extract timeline information from evidence."""
        # This would be enhanced with temporal analysis
        return "Analysis required - evidence insufficient for timeline reconstruction"
    
    def _generate_confidence_assessment(self) -> str:
        """Generate detailed confidence assessment."""
        if not self.evidence:
            return "No evidence available for assessment"
        
        scores = [e.get('confidence', 0.0) for e in self.evidence]
        avg_score = sum(scores) / len(scores)
        high_conf = len([s for s in scores if s >= 0.8])
        low_conf = len([s for s in scores if s < 0.5])
        
        assessment = f"Overall confidence: {avg_score:.2f}/1.00\n"
        assessment += f"- High confidence evidence: {high_conf}/{len(scores)} items\n"
        assessment += f"- Low confidence evidence: {low_conf}/{len(scores)} items\n"
        
        if avg_score >= 0.8:
            assessment += "- **Assessment: Highly reliable** - Strong evidence base"
        elif avg_score >= 0.6:
            assessment += "- **Assessment: Moderately reliable** - Good evidence base"
        else:
            assessment += "- **Assessment: Limited reliability** - Weak evidence base"
        
        return assessment
    
    def generate_evidence_section(self) -> str:
        """Generate detailed evidence section."""
        evidence_text = """## Evidence Analysis

### Evidence Summary
- **Total Items:** {total_count}
- **Source Documents:** {doc_count}
- **Date Range:** {date_range}
- **Evidence Quality:** {quality_assessment}

### Evidence Breakdown by Source
{source_breakdown}

### Evidence Breakdown by Confidence
{confidence_breakdown}

### Key Evidence Items
{key_evidence}

---
""".format(
            total_count=len(self.evidence),
            doc_count=len(set(e.get('document_id') for e in self.evidence)),
            date_range=self._get_date_range(),
            quality_assessment=self._assess_evidence_quality(),
            source_breakdown=self._generate_source_breakdown(),
            confidence_breakdown=self._generate_confidence_breakdown(),
            key_evidence=self._generate_key_evidence()
        )
        
        return evidence_text
    
    def _get_date_range(self) -> str:
        """Get date range from evidence."""
        # This would be enhanced with actual date extraction
        return "Analysis required - evidence insufficient for date range"
    
    def _assess_evidence_quality(self) -> str:
        """Assess overall evidence quality."""
        if not self.evidence:
            return "No evidence available"
        
        scores = [e.get('confidence', 0.0) for e in self.evidence]
        avg_score = sum(scores) / len(scores)
        
        if avg_score >= 0.8:
            return "EXCELLENT - High-quality, well-corroborated evidence"
        elif avg_score >= 0.6:
            return "GOOD - Reliable evidence with some corroboration"
        elif avg_score >= 0.4:
            return "FAIR - Adequate evidence with limited corroboration"
        else:
            return "POOR - Weak evidence requiring additional sources"
    
    def _generate_source_breakdown(self) -> str:
        """Generate breakdown of evidence by source."""
        sources = {}
        for e in self.evidence:
            source = e.get('source', 'unknown')
            if source not in sources:
                sources[source] = 0
            sources[source] += 1
        
        if not sources:
            return "No source information available"
        
        breakdown = ""
        for source, count in sources.items():
            breakdown += f"- **{source.title()}:** {count} items\n"
        
        return breakdown
    
    def _generate_confidence_breakdown(self) -> str:
        """Generate breakdown of evidence by confidence level."""
        confidence_levels = {
            'HIGH (0.8+)': 0,
            'MEDIUM (0.6-0.8)': 0,
            'LOW (0.4-0.6)': 0,
            'VERY LOW (<0.4)': 0
        }
        
        for e in self.evidence:
            conf = e.get('confidence', 0.0)
            if conf >= 0.8:
                confidence_levels['HIGH (0.8+)'] += 1
            elif conf >= 0.6:
                confidence_levels['MEDIUM (0.6-0.8)'] += 1
            elif conf >= 0.4:
                confidence_levels['LOW (0.4-0.6)'] += 1
            else:
                confidence_levels['VERY LOW (<0.4)'] += 1
        
        breakdown = ""
        for level, count in confidence_levels.items():
            if count > 0:
                breakdown += f"- **{level}:** {count} items\n"
        
        return breakdown
    
    def _generate_key_evidence(self) -> str:
        """Generate list of key evidence items."""
        if not self.evidence:
            return "No evidence available"
        
        # Sort by confidence and take top 5
        sorted_evidence = sorted(self.evidence, key=lambda x: x.get('confidence', 0.0), reverse=True)
        top_evidence = sorted_evidence[:5]
        
        key_items = ""
        for i, e in enumerate(top_evidence, 1):
            doc_id = e.get('document_id', 'unknown')
            page = e.get('page', 'unknown')
            confidence = e.get('confidence', 0.0)
            snippet = e.get('snippet', '')[:100] + "..." if len(e.get('snippet', '')) > 100 else e.get('snippet', '')
            
            key_items += f"{i}. **Document {doc_id}, Page {page}** (Confidence: {confidence:.2f})\n"
            key_items += f"   {snippet}\n\n"
        
        return key_items
    
    def generate_methodology_section(self) -> str:
        """Generate methodology section."""
        methodology = """## Methodology

### Analysis Approach
This threat actor profile was generated using the Orpheus CTI Agent, which employs a **deterministic-first approach with guarded LLM fallback** to ensure accuracy and reliability.

### Evidence Processing
1. **Document Ingestion:** PDF documents processed with OCR fallback for text extraction
2. **Chunking:** Documents segmented into 300-800 token chunks for analysis
3. **Vector Search:** FAISS-based semantic similarity search using OpenAI embeddings
4. **Keyword Search:** BM25-based lexical search for precise term matching
5. **Hybrid Fusion:** Combined results with MMR diversification for coverage
6. **Counter-Evidence:** Active search for contradictory evidence to reduce bias

### Quality Assurance
- **Citation Requirements:** Every claim must reference specific evidence
- **Confidence Scoring:** Multi-signal scoring (semantic + lexical + layout + entity + recency)
- **Policy Gates:** Configurable approval thresholds for publication
- **Audit Trails:** Complete logging of analysis decisions and evidence sources

### Limitations
- Analysis limited to ingested document corpus
- Confidence scores based on available evidence quality
- Timeline reconstruction requires temporal evidence
- Sector attribution requires explicit sector indicators

---
"""
        return methodology
    
    def generate_footer(self) -> str:
        """Generate report footer."""
        footer = f"""## Report Metadata

**Generated:** {self.timestamp.strftime('%Y-%m-%d %H:%M:%S UTC')}  
**Agent Version:** Orpheus CTI Agent  
**Evidence Sources:** {len(set(e.get('document_id') for e in self.evidence))} documents  
**Analysis Confidence:** {self._calculate_confidence_level()}  

**Contact:** For questions about this report, contact your CTI team.  
**Distribution:** This report is marked TLP {self.metadata.get('tlp', 'CLEAR')}.

---
*Generated by Argo - The Argonauts SOC Platform*
"""
        return footer


class ThreatProfileTemplate(CTIReportTemplate):
    """Template for threat actor profiles."""
    
    def generate_full_report(self) -> str:
        """Generate complete threat actor profile."""
        report = ""
        report += self.generate_header()
        report += self.generate_executive_summary()
        report += self.generate_evidence_section()
        report += self.generate_methodology_section()
        report += self.generate_footer()
        
        return report


class IncidentReportTemplate(CTIReportTemplate):
    """Template for incident reports."""
    
    def generate_full_report(self) -> str:
        """Generate complete incident report."""
        # Similar structure but focused on incident details
        report = ""
        report += self.generate_header()
        report += self.generate_executive_summary()
        report += self.generate_evidence_section()
        report += self.generate_methodology_section()
        report += self.generate_footer()
        
        return report


def get_report_template(template_type: str, actor: str, evidence: List[Dict[str, Any]], metadata: Dict[str, Any]) -> CTIReportTemplate:
    """Get appropriate report template."""
    if template_type == "threat_profile":
        return ThreatProfileTemplate(actor, evidence, metadata)
    elif template_type == "incident_report":
        return IncidentReportTemplate(actor, evidence, metadata)
    else:
        # Default to threat profile
        return ThreatProfileTemplate(actor, evidence, metadata)
